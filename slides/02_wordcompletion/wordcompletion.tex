\documentclass[professionalfonts, xcolor={usenames,svgnames,x11names,table}]{beamer}

\usetheme{SBUclass}
\usepackage{mypackages}
\usepackage{mycommands}


\title{Word Completion}
\author{Thomas Graf}
\institute{Stony Brook University\\\texttt{lin120@thomasgraf.net}}
\date{LIN 220, Spring 2019\\Lecture 2}


\begin{document}
\unnumbered{
\begin{frame}
	\titlepage
\end{frame}
}

\begin{frame}{Word completion: a simple problem(?)}
    \begin{itemize}
        \item When you write something on your phone,\\
              it automatically suggests words while you're typing them.
        \item Seems easy, but it's \highlight{fairly intricate}.
    \end{itemize}

    \begin{lessons}
        \begin{itemize}
            \item transition probabilities with n-grams
            \item ``More than 1, but less than 5.''
            \item efficient data structures: tries\slash prefix trees
        \end{itemize}
    \end{lessons}
\end{frame}

\begin{frame}[fragile]{Attempt 1: simple lookup}
    \begin{itemize}
        \item To make suggestions for completions,\\
              we only need an English\slash German\slash\ldots dictionary.
    \end{itemize}

    \begin{pythoncode}
        # load English dictionary from nltk package
        from nltk.corpus import words
        # and see if "test" is in the list
        "test" in words.words()
        >>> True
    \end{pythoncode}

    \begin{itemize}
        \item Given word \Purple{w}, the set of possible completions for \Purple{w}\\
              consists of all listed words that start with \Purple{w}:
    \end{itemize}

    \begin{align*}
        \text{completions}(\Purple{w}) = \{ \Teal{w'} \mid &
              \Teal{w'} \text{ is a word of English, and}\\
            & \Teal{w'} \text{ starts with } \Purple{w} \}
    \end{align*}
\end{frame}

\begin{frame}[fragile]{Attempt 1 [cont.]}
    \begin{itemize}
        \item completions(\Purple{w}) is easily computed in Python.
    \end{itemize}

    \begin{pythoncode}
        def completions(word, wordlist):
            """Return set of all known completions for word."""
            return {comp for comp in wordlist
                    if comp.startswith(word)}
    \end{pythoncode}

    \begin{pythoncode}
        completions("testing", words.words())
        >>> {"testing", "testingly"}
    \end{pythoncode}
\end{frame}

\begin{frame}[fragile]{Evaluating attempt 1}
    \begin{itemize}
        \item Is this a \highlight{good solution?}
    \end{itemize}

    \begin{center}
        \begin{tabular}{c@{\hspace{2em}}c}
            \toprule
            \textbf{Pro} & \textbf{Contra}\\
            \midrule
            \visible<2->{simple} & \visible<3->{too many completions}\\
                                 & \visible<4->{unlikely completions}\\
            \bottomrule
        \end{tabular}
    \end{center}

    \onslide<3->
    \begin{pythoncode}
        completions("test", words.words())
        >>> {"test", "testa", "testable", "testacean", ...}
    \end{pythoncode}
\end{frame}

\begin{frame}[fragile]{Improving attempt 1}
    \begin{itemize}
        \item We can limit the number of suggestions,\\
              if we use a \textbf{list} instead of a \textbf{set}.
    \end{itemize}
    \begin{pythoncode}
        def completions(word, wordlist):
            """Return list of all known completions for word."""
            return [comp for comp in wordlist
                    if comp.startswith(word)]

        completions("test", words.words())[:3]
        >>> ["test", "testa", "testable"]
    \end{pythoncode}
    \begin{itemize}
        \item But this still includes unlikely completions like \emph{testa}.
        \item We need \highlight{probabilities}!
    \end{itemize}
\end{frame}

\begin{frame}{Probability = frequency (?)}
    \begin{itemize}
        \item We only want to show the \highlight{most likely completions}.
        \item But what is most likely?
        \item \textbf{Naive proposal:} the most frequent word!
    \end{itemize}

    \visible<2->{%
        \begin{center}
            Um, but how do we figure out what is most common?
        \end{center}
    }
\end{frame}

\begin{frame}{How to find common words}
    \begin{itemize}
        \item \textbf{Task:} determine which words are common
        \item \textbf{Solution:}
            \begin{enumerate}
                \item Collect sufficiently large sample of texts
                \item For each word (\textbf{type}), count how often it occurs in\\
                    the entire sample (= its number of \textbf{tokens}).
                \item Calculate the \highlight{frequency} of the word in the sample:
                    \[
                        \text{freq}(
                            \text{\colored{blue!75}{\emph{word}}},
                            \text{\colored{orange}{\emph{sample}}}
                            )
                        =
                        \frac{%
                            \text{number of tokens of \colored{blue!75}{\emph{word}}}}
                            {\text{word length of whole \colored{orange}{\emph{sample}}}}
                    \]
            \end{enumerate}
    \end{itemize}

    \pause
    \begin{block}{Types vs tokens}
        We have to distinguish \textbf{\alert<3>{word} types}
        (\emph{a}, \emph{the}, \emph{Mary}, \emph{red}, \ldots)
        from their \textbf{\alert<3>{word} tokens}, which are the instances of a specific \alert<3>{word} type.
        For instance, the type ``\alert<3>{word}'' has \alert<3>{4 tokens} in this box.
    \end{block}
\end{frame}

\begin{frame}{Example calculation}
    \textbf{Sample:} \colored{SteelBlue4}{1000} words long
    \hfill
    \textbf{Words:} be, bed, bee, bell
    %
    \begin{center}
        \begin{tabular}{rcccc}
            \textbf{Type} & be & bed & bee & bell\\
            \textbf{Tokens} & \color{orange}\bfseries 13 & \color{purple}\bfseries 2 & \color{SeaGreen4}\bfseries 0 & \color{brown}\bfseries 3 
        \end{tabular}
    \end{center}
    %
    \[
        \begin{array}{c@{\hspace{2em}}c}
            \text{freq}(\text{be}) = \frac{\color{orange}\mathbf{13}}{\color{SteelBlue4} 1000} = 1.3\%
            &
            \text{freq}(\text{bee}) = \frac{\color{SeaGreen4}\mathbf{0}}{\color{SteelBlue4} 1000} = 0.0\%
            \\[6pt]
            \text{freq}(\text{bed}) = \frac{\color{purple}\mathbf{2}}{\color{SteelBlue4} 1000} = 0.2\%
            &
            \text{freq}(\text{bell}) = \frac{\color{brown}\mathbf{3}}{\color{SteelBlue4} 1000} = 0.3\%\\
        \end{array}
    \]

    \visible<2->{%
        \begin{center}
            Ordered completions for \emph{be}: \visible<3->{be}\visible<4->{, bell}\visible<5->{, bed}\visible<6>{, bee}
        \end{center}
    }
\end{frame}

\begin{frame}{Types of corpora}
    \begin{itemize}
        \item \textbf{Corpus} = large, structured collections of texts
        \begin{description}
            \item[mono-\slash multilingual] just one language, or many?
            \item[annotated] not just text, but additional annotations\\
                             (e.g. tags for part of speech, syntax trees)
        \end{description}
    \end{itemize}

    \begin{block}{Some common corpora in Python's NLTK}
        \begin{description}
            \item[Brown] 1 million words, tagged, 500 samples across 15 genres (fiction, news paper, \ldots)
            \item[Gutenberg] 1.8 million words, 18 classic texts of fiction
            \item[Penn] 40k words, tagged and parsed
            \item[Reuters] 1.3 million words, news documents
            \item[Switchboard] 36 phone calls, fully transcribed and parsed
            \item[Wordlist] 960k words (no repetitions) and 20k affixes\\
                            for 8 languages
        \end{description}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Getting probabilities from the corpus}
    \begin{pythoncode}
        from nltk.corpus import brown
        from collections import Counter

        # load Brown corpus as sequence of words
        brown_text = brown.words()
        # total number of words = length of text
        total = len(brown_text)
        # calculate counts
        brown_counts = Counter(brown_text)
        # convert counts to frequencies
        for word in brown_counts:
            brown_counts[word] /= total
    \end{pythoncode}

    \begin{itemize}
        \item Alright, we have frequencies for each word.\\
              Now what?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Ordering completions by frequency}
    \begin{itemize}
        \item For a good user experience, completions should appear in\\
              \highlight{descending order of probability}.
    \end{itemize}

    \begin{pythoncode}
        def completions(word, counts):
            """Return set of all known completions for word.

            The completions are sorted by frequency,
            in descending order.
            """
            comps = [comp for comp in counts
                     if comp.startswith(word)]
            return sorted(comps,
                          key=counts.get,
                          reverse=True)
    \end{pythoncode}

    \begin{pythoncode}
        completions("test", words.words(), brown_counts)
        >>> ["test", "testimony", "tested", "testing", ...]
    \end{pythoncode}
\end{frame}

\begin{frame}{Summary for revised attempt 1}
    \begin{enumerate}
        \item \textbf{Needed resources:} corpus
        \item Compute frequencies for all words in corpus
        \item Look up possible completions for user input
        \item Sort completions by their frequency
    \end{enumerate}

    \begin{center}
        Great, we're done, right?! Not quite\ldots
    \end{center}
\end{frame}

\begin{frame}{Probability $\neq$ word frequency}
    \begin{itemize}
        \item The probability of a word isn't fixed,
              it varies by \highlight{context}.
    \end{itemize}

    \begin{example}
        \begin{center}
            \begin{tabular}{c@{\hspace{2em}}ccc}
                & \textbf{tested} & \textbf{testing} & \textbf{testimony}\\
                \textbf{I have} & hi & low & mid\\
                \textbf{I have been} & hi & hi & low\\
                \textbf{I have the} & low & low & hi\\
            \end{tabular}
        \end{center}
    \end{example}

    \begin{itemize}
        \item The frequency of words is not enough,\\
              we need frequencies of sequences of words $\Rightarrow$ \highlight{n-grams}
    \end{itemize}
\end{frame}

\begin{frame}{Defining n-grams}
    \begin{description}
        \item[n-gram] a contiguous sequence of n words
    \end{description}

    \begin{center}
        \begin{tabular}{rll}
            \toprule
            \textbf{n} & \textbf{Name} & \textbf{Example}\\
            \midrule
            1 & unigram & John\\
            2 & bigram  & John to\\
            3 & trigram & John to be\\
            4 & 4-gram & John to be in\\
            5 & 5-gram & John to be in the\\
            \bottomrule
        \end{tabular}
    \end{center}

    \begin{example}
        \textbf{String}\\
        \begin{tikzpicture}
            \matrix (m) at (0,0) [matrix of nodes, ampersand replacement=\&] {%
                John \& and \& Marie \& are \& not \& Bill \& and \& Sue\\
            };
            \begin{pgfonlayer}{background}
                % bigrams
                \foreach \Right [remember=\Right as \previous (initially 1)] in {2,3,4,5,6,7,8}
                    \draw[draw=purple,fill=purple!25,thick,
                          visible on=<\Right>]
                        (m-1-\previous.south west) rectangle (m-1-\Right.north east);

                % trigrams
                \foreach \Left/\Right/\Timer in {%
                    1/3/9,
                    2/4/10,
                    3/5/11,
                    4/6/12,
                    5/7/13,
                    6/8/14%
                    }
                    \draw[draw=blue,fill=blue!25,thick,
                          visible on=<\Timer>]
                        (m-1-\Left.south west) rectangle (m-1-\Right.north east);

                % 4-grams
                \foreach \Left/\Right/\Timer in {%
                    1/4/15,
                    2/5/16,
                    3/6/17,
                    4/7/18,
                    5/8/19%
                    }
                    \draw[draw=brown,fill=brown!25,thick,
                          visible on=<\Timer>]
                        (m-1-\Left.south west) rectangle (m-1-\Right.north east);

                % 5-grams
                \foreach \Left/\Right/\Timer in {%
                    1/5/20,
                    2/6/21,
                    3/7/22,
                    4/8/23%
                    }
                    \draw[draw=teal,fill=teal!25,thick,
                          visible on=<\Timer>]
                        (m-1-\Left.south west) rectangle (m-1-\Right.north east);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{example}
\end{frame}

\begin{frame}{Frequencies for n-grams}
    Frequencies can be computed for n-grams, too.

    \begin{exampleblock}{Example: Calculating Bigram Frequencies}
        \begin{itemize}
            \item \textbf{String}\\
                \begin{tikzpicture}
                    \matrix (m) at (0,0) [matrix of nodes, ampersand replacement=\&] {%
                        when \& buffalo \& buffalo \& buffalo \& buffalo \& buffalo \& buffalo\\
                    };
                    \begin{pgfonlayer}{background}
                        \foreach \Right [remember=\Right as \previous (initially 1)] in {2,3,4,5,6,7}
                            \draw[draw=purple,fill=purple!25,thick,
                                  visible on=<\Right>]
                                (m-1-\previous.south west) rectangle (m-1-\Right.north east);
                    \end{pgfonlayer}
                \end{tikzpicture}
            %
            \item \textbf{Bigram token list}\\
                \visible<2->{when buffallo,}
                \visible<3->{buffalo buffalo,}
                \visible<4->{buffalo buffalo,}
                \visible<5->{buffalo buffalo,}
                \visible<6->{buffalo buffalo,}
                \visible<7->{buffalo buffalo}
            %
            \item \textbf{Bigram counts and frequencies}\\
                \visible<8>{
                \begin{enumerate}
                    \item when buffalo: 1 $\Rightarrow \frac{1}{6} = 16.7\%$
                    \item buffalo buffalo: 5 $\Rightarrow \frac{5}{6} = 83.3\%$
                \end{enumerate}
                }
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Adapting the strategy}
    \begin{enumerate}
        \item \textbf{Needed resources:} corpus
        \item Convert corpus to list of n-gram tokens
        \item Compute frequencies for all n-grams
        \item Look up possible completions for user input
        \item Look at previous $n-1$ words.
        \item Sort completions by n-gram probability
    \end{enumerate}

    \begin{example}
        \begin{itemize}
            \item \textbf{Trigram frequencies}\\
                \begin{tabular}{lr@{\hspace{2em}}lr}
                    bus is late  & 30\% & train is late  & 15\%\\
                    bus is lovely & 25\% & train is lovely & 8\%\\
                    bus is lazy & 10\% & train is lazy & 2\%\\
                \end{tabular}
        \end{itemize}

        \begin{columns}
            \column{.52\linewidth}
            \begin{itemize}
                \item \textbf{Input}\\
                    I will text you if the train is l
            \end{itemize}
            \column{.4\linewidth}
            \begin{itemize}
                \item \textbf{Sorted completions}\\
                    \visible<2->{late}%
                    \visible<3->{, lovely}%
                    \visible<4>{, lazy}
            \end{itemize}
        \end{columns}
    \end{example}
\end{frame}

\begin{frame}[fragile]{How it is done: The easy generalization step}
    \begin{pythoncode}
        def bigrams(text):
            """Convert text to list of bigram tokens."""
            return [text[n:n+2] for n in range(len(text) - 1)]

        
        brown.words()[:5]
        >>> ["The", "Fulton", "County", "Grand", "Jury"]
        bigrams(brown.words())[:3]
        >>> [["The", "Fulton"], ["Fulton", "County"],
             ["County", "Grand"]]
    \end{pythoncode}

    \begin{pythoncode}
        brown_bigrams = bigrams(brown.words())
        total = len(brown_bigrams)
        brown_bicounts = Counter(brown_bigrams)
        for bigram in brown_bicounts:
            brown_bicounts[bigram] /= total
    \end{pythoncode}
\end{frame}

\begin{frame}[fragile]{How it is done: The trickier part}
    \begin{pythoncode}
def bigram_completions(word, previous_word, counts):
    # set of all compatible bigrams
    comps = [comp for comp in counts
             if comp[:-1] == previous_word and
                comp[-1].startswith(word)]
    # sort the bigram completions
    ordered_ngrams = sorted(comps,
                            key=counts.get,
                            reverse=True)
    # only keep last word of each bigram
    return [ngram[-1] for ngram in ordered_ngrams]
    \end{pythoncode}
\end{frame}

\begin{frame}{Linguistic evaluation}
    \begin{itemize}
        \item We now use the local context to choose word completions.
    \end{itemize}

    \begin{block}{The n-Gram Hypothesis}
        One can reliably predict the next word based on\\
        the \highlight{preceding $n-1$ words}.
    \end{block}

    \begin{itemize}
        \item The n-gram hypothesis is \highlight{not quite true}, though.
    \end{itemize}
\end{frame}

\begin{frame}{Context matters a lot}
    \begin{itemize}
        \item Words do not exist in a vacuum.

        \begin{example}
            The word \emph{hypothyroidism} is rarely heard or seen,\\
            unless you're an endocrinologist.
        \end{example}

        \item Word choices depend greatly on genre, target audience,\\
              age of the speaker, and so on.
    \end{itemize}

    \begin{block}{Common fixes}
        \begin{itemize}
            \item Use frequencies from the appropriate genre\\
                  (\textbf{BUT:} must be able to reliably determine genre first)
            \item Learn directly from the use case\\
                  (e.g.~analyze all text messages on phone)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Long-distance dependencies in language}
    \begin{itemize}
        \item Word choice can be influenced by words that are very far away.
    \end{itemize}

    \begin{exampleblock}{Subject-verb agreement}
        \begin{itemize}
            \item The key to the cabinet \textbf{is} on the table.
            \item The keys to the cabinets \textbf{are} on the table.
            \item The key to the cabinets \textbf{is}\slash\highlight{are} on the table.
            \item The keys to the cabinet \highlight{is}\slash\textbf{are} on the table.
        \end{itemize}
    \end{exampleblock}

    \begin{itemize}
        \item Psycholinguistic observation: humans get those ``wrong'' too
        \item Potential fix: \highlight{larger n-grams}
    \end{itemize}
\end{frame}

\begin{frame}{How long can n-grams be?}
    \begin{itemize}
        \item It is tempting to move to longer and longer n-grams\\
              in order to handle long-distance dependencies.
        \item But this has \highlight{two problems}:
                \begin{description}
                    \item[data sparsity] longer n-grams require too much data
                    \item[storage needs] longer n-grams require lots of storage
                \end{description}
        \item Data sparsity is much more severe than storage needs.
    \end{itemize}    
\end{frame}

\begin{frame}{Sparse data: A simple calculation}
    \begin{center}
        \small
        \begin{tabular}{rcccc}
            \textbf{Words} &\textbf{bigrams} & \textbf{trigrams} & \textbf{5-grams} & \textbf{6-grams}\\
            10 & 100 & 1000 & 10,000 & 100,000\\
            100 & 10,000 & 1,000,000 & 10,000,000,000 & \Purple{1,000,000,000,000}\\
            10,000 & 10$^8$ & \Purple{10$^{12}$} & 10$^{20}$ & 10$^{24}$\\
            25,000 & $6.3 \times 10^8$ & $1.6 \times 10^{13}$ & $9.7 \times 10^{21}$ & $2.4 \times 10^{26}$\\
        \end{tabular}
    \end{center}

    \begin{block}{Some comparison values}
        \begin{description}
            \item[$4.3 \times 10^{17}$] number of seconds since the Big Bang
            \item[$5 \times 10^{22}$]   number of stars in observable universe
            \item[$10^{24}$] milliliters of water in the Earth's oceans\\
            \item[$8.8 \times 10^{26}$] diameter of observable universe, in meters
            \item[$10^{80}$] number of atoms in observable universe
        \end{description}
    \end{block}

    \begin{itemize}
        \item \textbf{Conclusion:}
               with large n, most n-grams are\\
               \highlight{never encountered} in a corpus
               $\Rightarrow$ frequency 0
    \end{itemize}
\end{frame}

\begin{frame}{Things get worse: A more realistic estimate}
    \begin{itemize}
        \item The Linux dictionary american-english-insane has\\
            650,000 entries.
        \item This makes the numbers much worse.\\
            Can you guess how many 5-grams there are then?
    \end{itemize}

    \begin{center}
        \visible<2->{116 octillion $\mathbf{\approx 10^{29}}$}
    \end{center}
        
    \medskip
    \visible<3>{
            $10^{29}$ is larger than the number of shotglasses it takes to\\
            drain the Earth's oceans over 2000 times.
        }
\end{frame}

\begin{frame}{Trick 1: Stemming and lemmatization}
    \begin{itemize}
        \item Removing inflectional markers reduces number of words
        \item Two solutions:
            \begin{itemize}
                \item stemming is quick and dirty
                \item lemmatization is accurate but complex
            \end{itemize}
    \end{itemize}

    \begin{description}
        \item[stemming] cut off word ends that look like inflection
    \end{description}

    \begin{example}
        \begin{itemize}
            \item cats $\Rightarrow$ cat
            \item tasks $\Rightarrow$ task (noun and verb)
            \item asking $\Rightarrow$ ask
            \item meeting $\Rightarrow$ meet (\textbf{noun and verb})
            \item staging $\Rightarrow$ stag
        \end{itemize}
    \end{example}
\end{frame}

\begin{frame}{Trick 1: Stemming and lemmatization [cont.]}
    \begin{description}
        \item[lemmatization] stemming with context information
    \end{description}

    \begin{example}
        \begin{itemize}
            \item cats $\Rightarrow$ cat
            \item tasks $\Rightarrow$ task (noun and verb)
            \item asking $\Rightarrow$ ask
            \item meeting $\Rightarrow$ meet (\textbf{only verb})
            \item staging $\Rightarrow$ stag\slash stage
        \end{itemize}
    \end{example}

    \begin{block}{Evaluation}
        \begin{itemize}
            \item Stemming\slash lemmatization reduces the number of words.
            \item But we still have at least 10,000 words and thus $10^{20}$ 5-grams.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Trick 2: POS tagging}
    \begin{itemize}
        \item Every word has a part of speech (POS).
        \item If we know the POS of a word, we can use that for\\
              estimating probabilities.
    \end{itemize}

    \begin{example}
        \begin{itemize}
            \item \textbf{User input:} an
            \item \textbf{Preceding words:} to
            \item \textbf{Completions:} an (Det), annoint (V)
            \item \textbf{Suggestion:} to annoint
            \item \textbf{Reasoning:}
                    even though \emph{to an} is more common than \emph{to annoint}, 
                    \emph{to V} is more common than \emph{to Det}.
        \end{itemize}
    \end{example}
\end{frame}

\begin{frame}{Trick 3: Statistics}
    \begin{itemize}
        \item \textbf{Backoff Method}\\
            If an n-gram has frequency $0$, use the frequency of the corresponding $(n-1)$-gram.
        \item \textbf{Good-Turing Smoothing}\\
            Change frequency from $0$ to a very low value while lowering high frequency values.
    \end{itemize}

    \begin{block}{Evaluation}
        \begin{itemize}
            \item These tricks solve the issue of n-grams with 0\% frequency.
            \item But they do not solve the basic problem that\\
                  large n-gram models are incredibly data hungry.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Another problem: Storage}
    \begin{itemize}
        \item While storage space is increasing with each year,\\
              you don't want to waste GB on storing n-gram frequencies.
        \item We want a storage solution that is
                \begin{itemize}
                    \item compact,
                    \item easy to expand,
                    \item \highlight{fast to search}.
                \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Searching through a list}
    List search is a much studied problem.
    %
    \begin{block}{Simplest Algorithm: Linear Search}
        \begin{itemize}
            \item Start at beginning of list.
            \item Move from left to right until the wanted item has been found.
            \item If no match is found before end of list, item is not in list.
        \end{itemize}
    \end{block}

    \pause
    \textbf{Searching for \emph{yogurt}}
    %
    \begin{center}
        \begin{tikzpicture}
            \foreach \Label/\Pos in {0/0,1/5.5,2/11,3/16.5,4/22,5/27.5}
                \node[draw=blue!75,fill=blue!25,circle,thick] (\Label) at (\Pos em, 0) {\Label};

            \foreach \Start/\Target/\Label in {0/1/'apple',1/2/'banana',2/3/'orange',3/4/'yogurt',4/5/'zoo'}
                \draw[blue!75,->] (\Start) to node [above] {\Label} (\Target);

            \foreach \Time/\Node in {3/0,4/1,5/2,6/3}
                \draw[visible on=<\Time>,->,red,very thick] ($(\Node.south)-(0,2em)$) to (\Node.south);
        \end{tikzpicture}
    \end{center}
\end{frame}

\begin{frame}{Evaluating linear search}
    \begin{itemize}
        \item Two criteria for evaluating algorithms:\\
            correctness and efficiency
        \item An algorithm is \highlight{correct} if and only if\\
              it is both sound and complete.
            %
            \begin{description}
                \item[sound] algorithm only returns correct solutions
                \item[complete] algorithm can find all correct solutions
            \end{description}
        \item Linear search is correct.
                \begin{itemize}
                    \item if item is not in list, linear search will not claim otherwise
                    \item if item is in list, linear search will find it
                \end{itemize}
        \item But it is also slow.
                \begin{itemize}
                    \item In a list with $n$ items, it may take $n$ steps to find item.
                    \item And we have tons of n-grams!
                \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Improving on lists: Prefix trees\slash Tries}
    \begin{block}{Prefix Trees: The Basic Idea}
        Exploit the fact that many items share a common start
    \end{block}

    \begin{center}
        \small
        \begin{tikzpicture}[
            stop/.style = {circle,fill=blue!25, minimum size=2em},
            cont/.style = {circle,fill=red!25, minimum size=2em}
            ]

            \node[cont] (root) at (0,0) {};
            \node [above=.1em of root, font=\bfseries\footnotesize] {Example: Prefix tree for word list};
            \foreach \Source/\Arc/\Label/\Status/\Offset in {%
                                                            root/a/a/stop/-8,
                                                            root/b/b/cont/8,
                                                            a/n/an/stop/-5,
                                                            a/l/al/cont/0,
                                                            a/t/at/stop/5,
                                                            al/l/all/stop/0,
                                                            b/e/be/stop/-4.5,
                                                            b/i/bi/stop/4.5,
                                                            be/d/bed/stop/-2.5,
                                                            be/e/bee/stop/2.5,
                                                            bi/d/bid/stop/-3.5,
                                                            bi/n/bin/stop/0,
                                                            bi/t/bit/stop/3.5%
                                                        }
                {
                    \node[\Status] (\Label) at ($(\Source)+(\Offset em,-4em)$) {\Label};
                    \draw[->] (\Source) to node [left] {\Arc} (\Label);
                }
        \end{tikzpicture}
    \end{center}
\end{frame}

\begin{frame}{Searching through a prefix tree}
    \textbf{Query:} Is word $w$ in our dictionary?\\
    \textbf{Algorithm:} follow branches that spell $w$; do we finish in \\
    accepting (= blue) node?
    %
    \begin{example}
        \begin{enumerate}
            \item \textbf{Query:} bit\\
                  \textbf{Search Path:} b-i-t, \colored{blue!75}{blue} $\Rightarrow$ \colored{blue!75}{accept}
                  %
            \item \textbf{Query:} al\\
                  \textbf{Search Path:} al, \colored{red!75}{red} $\Rightarrow$ \colored{red!75}{reject}
                  %
            \item \textbf{Query:} bits\\
                  \textbf{Search Path:} b-i-t, \colored{red!75}{stuck} $\Rightarrow$ \colored{red!75}{reject}
        \end{enumerate}
    \end{example}
\end{frame}

\begin{frame}{Prefix tree for n-grams}
    \begin{itemize}
        \item A prefix tree for a list of n-grams works exactly the same way,
              except that each arc is \highlight{labeled with a word} instead. 
    \end{itemize}

    \begin{center}
        \begin{tikzpicture}[
            stop/.style = {circle,fill=blue!25, minimum size=2em},
            cont/.style = {circle,fill=red!25, minimum size=2em}
            ]

            \node[cont] (root) at (0,0) {};
            \node [above left=.1em and -2.5em of root, font=\bfseries\footnotesize]
                  {Example: Prefix tree for trigram frequencies};
            \foreach \Source/\Arc/\Label/\Status/\Offset in {%
                                                            root/a//cont/-8,
                                                            root/to//cont/8,
                                                            a/new//cont/-7,
                                                            a/joy//cont/0,
                                                            a/bug//cont/7,
                                                            new/car/.07/stop/-3.5,
                                                            new/house/.05/stop/3.5,
                                                            joy/ride/.3/stop/0,
                                                            joy/in/.05/stop/3,
                                                            bug/seems/.03/stop/0,
                                                            to/the//cont/0,
                                                            the/Batmobile/.5/stop/0%
                                                        }
                {
                    \node[\Status] (\Arc) at ($(\Source)+(\Offset em,-4em)$) [font=\small] {\Label};
                    \draw[->] (\Source) to node [left] {\Arc} (\Arc);
                }
        \end{tikzpicture}
    \end{center}
\end{frame}

\begin{frame}{A mixed prefix tree}
    \begin{itemize}
        \item Prefix trees can store frequencies for all $m < n$
        \item useful for backoff methods
    \end{itemize}
    \begin{center}
        \only<1>{%
        \begin{tikzpicture}[
            stop/.style = {circle,fill=blue!25, minimum size=2em},
            cont/.style = {circle,fill=red!25, minimum size=2em}
            ]

            \node[cont] (root) at (0,0) {};
            \node [above left=.1em and -2.5em of root, font=\bfseries\footnotesize]
                  {Example: Prefix tree for trigram frequencies};
            \foreach \Source/\Arc/\Label/\Status/\Offset in {%
                                                            root/a//cont/-8,
                                                            root/to//cont/8,
                                                            a/new//cont/-7,
                                                            a/joy//cont/0,
                                                            a/bug//cont/7,
                                                            new/car/.07/stop/-3.5,
                                                            new/house/.05/stop/3.5,
                                                            joy/ride/.3/stop/0,
                                                            joy/in/.05/stop/3,
                                                            bug/seems/.03/stop/0,
                                                            to/the//cont/0,
                                                            the/Batmobile/.5/stop/0%
                                                        }
                {
                    \node[\Status] (\Arc) at ($(\Source)+(\Offset em,-4em)$) [font=\small] {\Label};
                    \draw[->] (\Source) to node [left] {\Arc} (\Arc);
                }
        \end{tikzpicture}}
        \only<2>{%
        \begin{tikzpicture}[
            stop/.style = {circle,fill=blue!25, minimum size=2em},
            cont/.style = {circle,fill=red!25, minimum size=2em}
            ]

            \node[cont] (root) at (0,0) {};
            \node [above left=.1em and -4.5em of root, font=\bfseries\footnotesize]
                  {Example: Prefix tree for bigram and trigram frequencies};
            \foreach \Source/\Arc/\Label/\Status/\Offset in {%
                                                            root/a//cont/-8,
                                                            root/to//cont/8,
                                                            a/new/.3/stop/-7,
                                                            a/joy/.2/stop/0,
                                                            a/bug/.1/stop/7,
                                                            new/car/.07/stop/-3.5,
                                                            new/house/.05/stop/3.5,
                                                            joy/ride/.3/stop/0,
                                                            joy/in/.05/stop/3,
                                                            bug/seems/.03/stop/0,
                                                            to/the/.5/stop/0,
                                                            the/Batmobile/.5/stop/0%
                                                        }
                {
                    \node[\Status] (\Arc) at ($(\Source)+(\Offset em,-4em)$) [font=\small] {\Label};
                    \draw[->] (\Source) to node [left] {\Arc} (\Arc);
                }
        \end{tikzpicture}}%
        \only<3>{%
        \begin{tikzpicture}[
            stop/.style = {circle,fill=blue!25, minimum size=2em},
            cont/.style = {circle,fill=red!25, minimum size=2em}
            ]

            \node[cont] (root) at (0,0) {};
            \node [above left=.1em and -7.5em of root, font=\bfseries\footnotesize]
                  {Example: Prefix tree for unigram, bigram and trigram frequencies};
            \foreach \Source/\Arc/\Label/\Status/\Offset in {%
                                                            root/a/.7/stop/-8,
                                                            root/to/.3/stop/8,
                                                            a/new/.3/stop/-7,
                                                            a/joy/.2/stop/0,
                                                            a/bug/.1/stop/7,
                                                            new/car/.07/stop/-3.5,
                                                            new/house/.05/stop/3.5,
                                                            joy/ride/.3/stop/0,
                                                            joy/in/.05/stop/3,
                                                            bug/seems/.03/stop/0,
                                                            to/the/.5/stop/0,
                                                            the/Batmobile/.5/stop/0%
                                                        }
                {
                    \node[\Status] (\Arc) at ($(\Source)+(\Offset em,-4em)$) [font=\small] {\Label};
                    \draw[->] (\Source) to node [left] {\Arc} (\Arc);
                }
        \end{tikzpicture}
        }
    \end{center}
\end{frame}

\begin{frame}{You try it!}
    Below is a short text.
    Do the following:
    \begin{enumerate}
        \item Tokenize the text (removing punctuation and capitalization).
        \item Draw a prefix tree that encodes the bigram and trigram frequencies.
        \item You do not need to fully calculate the frequencies.
              Fractions are enough (e.g. $\frac{3}{17}$).
    \end{enumerate}

    \begin{quote}
        Of course a powerful institution --- which the police is ---
        can convince the politicians that a police police to police the police is bad policy.
    \end{quote}

    % solution
    % ========
    % of - course - a
    % course - a - powerful
    % a - powerful - institution
    %   - police - police
    % powerful - institution - which
    % institution - which - the
    % which - the - police
    % the - police - is
    %     - politicians - that
    % police - is - can
    %             - bad
    %        - police - to
    %        - to - police
    %        - the - police
    % is - can - convince
    %    - bad - policy
    % can - convince - the
    % convince - the - politicians
    % politicians - that - a
    % that - a - police
\end{frame}

\begin{frame}{Excursus: It's the software, stupid!}
    \begin{itemize}
        \item We usually think of technological progress in terms of faster hardware, more memory, and so on.
        \item A good \highlight{algorithm trumps hardware} improvements.
    \end{itemize}

    \begin{example}
        \begin{itemize}
            \item Even the fastest current gen processor will take a long time\\
                to do linear search on a list with 18 quintillion items.
            \item With prefix trees it is almost instantaneous (and incremental!).
            \item They are also more memory efficient,\\
                  as long as your items have lots of overlap.
            \item \textbf{BUT:} They are harder to implement.
        \end{itemize}
    \end{example}
\end{frame}

\begin{frame}{Yet another problem: bad input}
    \begin{itemize}
        \item What if a user makes a mistake?
    \end{itemize}
    \begin{center}
        \begin{tabular}{c@{\hspace{2em}}c}
            \toprule
            \textbf{Input} & \textbf{Intended?}\\
            \midrule
            trian & train, trying, Brian, \ldots\\
            your & your, you're\\
            @rdvark & aardvark\\
            \bottomrule
        \end{tabular}
    \end{center}
    \begin{itemize}
        \item This requires a spellchecker.\\
              But more on that next time\ldots
    \end{itemize}
\end{frame}

\begin{frame}{Summary}
    A good word completion model needs \highlight{lots of components}:

    \begin{itemize}
        \item corpora from various genres
        \item n-grams for limited context information
        \item sophisticated statistics to deal with sparse data
        \item a fast and compact data structure for storage
        \item extensions:
            \begin{itemize}
                \item stemming\slash lemmatizer
                \item POS tagging
                \item spell checker
                \item efficient storage
            \end{itemize}
    \end{itemize}

    We have barely scratched the surface of any of those.
\end{frame}

\begin{frame}[fragile]{New concepts}
    \begin{itemize}
        \item corpora (common words, what types there are)
        \item n-grams (word = unigram)
        \item types VS tokens
        \item data sparsity
        \item smoothing and backoff
        \item stemming, lemmatization
        \item POS tagging
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{New Python concepts}
    \begin{pythoncode}
        sorted(some_list,
               # sort in reverse?
               reverse=True, 
               # what property to use for sorting
               key=some_function)

        range(n)  # all numbers from 0 to n-1, in order

        some_string.startswith(x)  # does some_string start with x?
    \end{pythoncode}
\end{frame}
\end{document}

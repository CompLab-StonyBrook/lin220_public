\documentclass[professionalfonts, xcolor={usenames,svgnames,x11names,table}]{beamer}

\usetheme{SBUclass}
\usepackage{mypackages}
\usepackage{mycommands}


\title{Word Completion}
\author{Thomas Graf}
\institute{Stony Brook University\\\texttt{lin120@thomasgraf.net}}
\date{LIN 220, Spring 2019\\Lecture 2}


\begin{document}
\unnumbered{
\begin{frame}
	\titlepage
\end{frame}
}

\begin{frame}{Word completion: a simple problem(?)}
    \begin{itemize}
        \item When you write something on your phone,\\
              it automatically suggests words while you're typing them.
        \item Seems easy, but it's \highlight{fairly intricate}.
    \end{itemize}

    \begin{lessons}
        \begin{itemize}
            \item transition probabilities with n-grams
            \item ``More than 1, but less than 5.''
            \item efficient data structures: tries\slash prefix trees
        \end{itemize}
    \end{lessons}
\end{frame}

\begin{frame}[fragile]{Attempt 1: simple lookup}
    \begin{itemize}
        \item To make suggestions for completions,\\
              we only need an English\slash German\slash\ldots dictionary.
    \end{itemize}

    \begin{pythoncode}
        # load English dictionary from nltk package
        from nltk.corpus import words
        # and see if "test" is in the list
        "test" in words.words()
        >>> True
    \end{pythoncode}

    \begin{itemize}
        \item Given word \Purple{w}, the set of possible completions for \Purple{w}\\
              consists of all listed words that start with \Purple{w}:
    \end{itemize}

    \[
        \text{completions}(\Purple{w}) =
            \{ w' \mid w' \text{ starts with } \Purple{w} \}
    \]
\end{frame}

\begin{frame}[fragile]{Attempt 1 [cont.]}
    \begin{itemize}
        \item completions(\Purple{w}) is easily computed in Python.
    \end{itemize}

    \begin{pythoncode}
        def completions(word, wordlist):
            """Return set of all known completions for word."""
            return {comp for comp in wordlist
                    if comp.startswith(word)}
    \end{pythoncode}

    \begin{pythoncode}
        completions("testing", words.words())
        >>> {"testing", "testingly"}
    \end{pythoncode}
\end{frame}

\begin{frame}[fragile]{Evaluating attempt 1}
    \begin{itemize}
        \item Is this a \highlight{good solution?}
    \end{itemize}

    \begin{center}
        \begin{tabular}{c@{\hspace{2em}}c}
            \toprule
            \textbf{Pro} & \textbf{Contra}\\
            \midrule
            \visible<2->{simple} & \visible<3->{too many completions}\\
                                 & \visible<4->{unlikely completions}\\
            \bottomrule
        \end{tabular}
    \end{center}

    \onslide<3->
    \begin{pythoncode}
        completions("test", words.words())
        >>> {"test", "testa", "testable", "testacean", ...}
    \end{pythoncode}
\end{frame}

\begin{frame}[fragile]{Improving attempt 1}
    \begin{itemize}
        \item We can limit the number of suggestions,\\
              if we use a \textbf{list} instead of a \textbf{set}.
    \end{itemize}
    \begin{pythoncode}
        def completions(word, wordlist):
            """Return list of all known completions for word."""
            return [comp for comp in wordlist
                    if comp.startswith(word)]

        completions("test", words.words())[:3]
        >>> ["test", "testa", "testable"]
    \end{pythoncode}
    \begin{itemize}
        \item But this still includes unlikely completions like \emph{testa}.
        \item We need \highlight{probabilities}!
    \end{itemize}
\end{frame}

\begin{frame}{Probability = Frequency (?)}
    \begin{itemize}
        \item We only want to show the \highlight{most likely completions}.
        \item But what is most likely?
        \item \textbf{Naive proposal:} the most frequent word!
    \end{itemize}

    \visible<2->{%
        \begin{center}
            Um, but how do we figure out what is most common?
        \end{center}
    }
\end{frame}

\begin{frame}{How to find common words}
    \begin{itemize}
        \item \textbf{Task:} determine which words are common
        \item \textbf{Solution:}
            \begin{enumerate}
                \item Collect sufficiently large sample of texts
                \item For each word (\textbf{type}), count how often it occurs in\\
                    the entire sample (= its number of \textbf{tokens}).
                \item Calculate the \highlight{frequency} of the word in the sample:
                    \[
                        \text{freq}(
                            \text{\colored{blue!75}{\emph{word}}},
                            \text{\colored{orange}{\emph{sample}}}
                            )
                        =
                        \frac{%
                            \text{number of tokens of \colored{blue!75}{\emph{word}}}}
                            {\text{word length of whole \colored{orange}{\emph{sample}}}}
                    \]
            \end{enumerate}
    \end{itemize}

    \pause
    \begin{block}{Types vs tokens}
        We have to distinguish \textbf{\alert<3>{word} types}
        (\emph{a}, \emph{the}, \emph{Mary}, \emph{red}, \ldots)
        from their \textbf{\alert<3>{word} tokens}, which are the instances of a specific \alert<3>{word} type.
        For instance, the type ``\alert<3>{word}'' has \alert<3>{4 tokens} in this box.
    \end{block}
\end{frame}

\begin{frame}{Example calculation}
    \textbf{Sample:} \colored{SteelBlue4}{1000} words long
    \hfill
    \textbf{Words:} be, bed, bee, bell
    %
    \begin{center}
        \begin{tabular}{rcccc}
            \textbf{Type} & be & bed & bee & bell\\
            \textbf{Tokens} & \color{orange}\bfseries 13 & \color{purple}\bfseries 2 & \color{SeaGreen4}\bfseries 0 & \color{brown}\bfseries 3 
        \end{tabular}
    \end{center}
    %
    \[
        \begin{array}{c@{\hspace{2em}}c}
            \text{freq}(\text{be}) = \frac{\color{orange}\mathbf{13}}{\color{SteelBlue4} 1000} = 1.3\%
            &
            \text{freq}(\text{bee}) = \frac{\color{SeaGreen4}\mathbf{0}}{\color{SteelBlue4} 1000} = 0.0\%
            \\[6pt]
            \text{freq}(\text{bed}) = \frac{\color{purple}\mathbf{2}}{\color{SteelBlue4} 1000} = 0.2\%
            &
            \text{freq}(\text{bell}) = \frac{\color{brown}\mathbf{3}}{\color{SteelBlue4} 1000} = 0.3\%\\
        \end{array}
    \]

    \visible<2->{%
        \begin{center}
            Ordered completions for \emph{be}: \visible<3->{be}\visible<4->{, bell}\visible<5->{, bed}\visible<6>{, bee}
        \end{center}
    }
\end{frame}

\begin{frame}{Types of corpora}
    \begin{itemize}
        \item \textbf{Corpus} = large, structured collections of texts
        \begin{description}
            \item[mono-\slash multilingual] just one language, or many?
            \item[annotated] not just text, but additional annotations\\
                             (e.g. tags for part of speech, syntax trees)
        \end{description}
    \end{itemize}

    \begin{block}{Some common corpora}
        \begin{description}
            \item[Brown] 1 million words, tagged, 500 samples across 15 genres (fiction, news paper, \ldots)
            \item[Gutenberg] 1.8 million words, 18 classic texts of fiction
            \item[Penn] 40k words, tagged and parsed
            \item[Reuters] 1.3 million words, news documents
            \item[Switchboard] 36 phone calls, fully transcribed and parsed
            \item[Wordlist] 960k words (no repetitions) and 20k affixes for 8 languages
        \end{description}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Getting probabilities from the corpus}
    \begin{pythoncode}
        from nltk.corpus import brown
        from collections import Counter

        # load Brown corpus as sequence of words
        brown_text = brown.words()
        # total number of words = length of text
        total = len(brown_text)
        # calculate counts
        brown_counts = Counter(brown_text)
        # convert counts to frequencies
        for word in brown_counts:
            brown_counts[word] = brown_counts[word]/total
    \end{pythoncode}

    \begin{itemize}
        \item Alright, we have frequencies for each word.\\
              Now what?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Ordering completions by frequency}
    \begin{itemize}
        \item For a good user experience, completions should appear in\\
              \highlight{descending order of probability}.
    \end{itemize}

    \begin{pythoncode}
        def completions(word, counts):
            """Return set of all known completions for word.

            The completions are sorted by frequency,
            in descending order.
            """
            comps = [comp for comp in counts
                     if comp.startswith(word)]
            return sorted(comps,
                          key=lambda x: counts[x],
                          reverse=True)
    \end{pythoncode}

    \begin{pythoncode}
        completions("test", words.words(), brown_counts)
        >>> ["test", "testimony", "tested", "testing", ...]
    \end{pythoncode}
\end{frame}

\begin{frame}{Summary for revised attempt 1}
    \begin{enumerate}
        \item \textbf{Needed resources:} corpus
        \item Compute frequencies for all words in corpus
        \item Look up possible completions for user input
        \item Sort completions by their frequency
    \end{enumerate}

    \begin{center}
        Great, we're done, right?! Not quite\ldots
    \end{center}
\end{frame}

\begin{frame}{Probability $\neq$ word frequency}
    \begin{itemize}
        \item The probability of a word isn't fixed,
              it varies by \highlight{context}.
    \end{itemize}

    \begin{example}
        \begin{center}
            \begin{tabular}{c@{\hspace{2em}}ccc}
                & \textbf{tested} & \textbf{testing} & \textbf{testimony}\\
                \textbf{I have} & hi & low & mid\\
                \textbf{I have been} & hi & hi & low\\
                \textbf{I have the} & low & low & hi\\
            \end{tabular}
        \end{center}
    \end{example}

    \begin{itemize}
        \item The frequency of words is not enough,\\
              we need frequencies of sequences of words $\Rightarrow$ \highlight{n-grams}
    \end{itemize}
\end{frame}

\begin{frame}{Defining n-grams}
    \begin{description}
        \item[n-gram] a contiguous sequence of n words
    \end{description}

    \begin{center}
        \begin{tabular}{rll}
            \toprule
            \textbf{n} & \textbf{Name} & \textbf{Example}\\
            \midrule
            1 & unigram & John\\
            2 & bigram  & John to\\
            3 & trigram & John to be\\
            4 & 4-gram & John to be in\\
            5 & 5-gram & John to be in the\\
            \bottomrule
        \end{tabular}
    \end{center}

    \begin{example}
        \textbf{String}\\
        \begin{tikzpicture}
            \matrix (m) at (0,0) [matrix of nodes, ampersand replacement=\&] {%
                John \& and \& Marie \& are \& not \& Bill \& and \& Sue\\
            };
            \begin{pgfonlayer}{background}
                % bigrams
                \foreach \Right [remember=\Right as \previous (initially 1)] in {2,3,4,5,6,7,8}
                    \draw[draw=purple,fill=purple!25,thick,
                          visible on=<\Right>]
                        (m-1-\previous.south west) rectangle (m-1-\Right.north east);

                % trigrams
                \foreach \Left/\Right/\Timer in {%
                    1/3/9,
                    2/4/10,
                    3/5/11,
                    4/6/12,
                    5/7/13,
                    6/8/14%
                    }
                    \draw[draw=blue,fill=blue!25,thick,
                          visible on=<\Timer>]
                        (m-1-\Left.south west) rectangle (m-1-\Right.north east);

                % 4-grams
                \foreach \Left/\Right/\Timer in {%
                    1/4/15,
                    2/5/16,
                    3/6/17,
                    4/7/18,
                    5/8/19%
                    }
                    \draw[draw=brown,fill=brown!25,thick,
                          visible on=<\Timer>]
                        (m-1-\Left.south west) rectangle (m-1-\Right.north east);

                % 5-grams
                \foreach \Left/\Right/\Timer in {%
                    1/5/20,
                    2/6/21,
                    3/7/22,
                    4/8/23%
                    }
                    \draw[draw=teal,fill=teal!25,thick,
                          visible on=<\Timer>]
                        (m-1-\Left.south west) rectangle (m-1-\Right.north east);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{example}
\end{frame}

\begin{frame}{Frequencies for n-grams}
    Frequencies can be computed for n-grams, too.

    \begin{exampleblock}{Example: Calculating Bigram Frequencies}
        \begin{itemize}
            \item \textbf{String}\\
                \begin{tikzpicture}
                    \matrix (m) at (0,0) [matrix of nodes, ampersand replacement=\&] {%
                        when \& buffalo \& buffalo \& buffalo \& buffalo \& buffalo \& buffalo\\
                    };
                    \begin{pgfonlayer}{background}
                        \foreach \Right [remember=\Right as \previous (initially 1)] in {2,3,4,5,6,7}
                            \draw[draw=purple,fill=purple!25,thick,
                                  visible on=<\Right>]
                                (m-1-\previous.south west) rectangle (m-1-\Right.north east);
                    \end{pgfonlayer}
                \end{tikzpicture}
            %
            \item \textbf{Bigram token list}\\
                \visible<2->{when buffallo,}
                \visible<3->{buffalo buffalo,}
                \visible<4->{buffalo buffalo,}
                \visible<5->{buffalo buffalo,}
                \visible<6->{buffalo buffalo,}
                \visible<7->{buffalo buffalo}
            %
            \item \textbf{Bigram counts and frequencies}\\
                \visible<8>{
                \begin{enumerate}
                    \item when buffalo: 1 $\Rightarrow \frac{1}{6} = 16.7\%$
                    \item buffalo buffalo: 5 $\Rightarrow \frac{5}{6} = 83.3\%$
                \end{enumerate}
                }
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Adapting the strategy}
    \begin{enumerate}
        \item \textbf{Needed resources:} corpus
        \item Convert corpus to list of n-gram tokens
        \item Compute frequencies for all n-grams
        \item Look up possible completions for user input
        \item Look at previous $n-1$ words.
        \item Sort completions by n-gram probability
    \end{enumerate}

    \begin{example}
        \begin{itemize}
            \item \textbf{Trigram frequencies}\\
                \begin{tabular}{lr@{\hspace{2em}}lr}
                    bus is late  & 30\% & train is late  & 15\%\\
                    bus is lovely & 25\% & train is lovely & 8\%\\
                    bus is lazy & 10\% & train is lazy & 2\%\\
                \end{tabular}
        \end{itemize}

        \begin{columns}
            \column{.52\linewidth}
            \begin{itemize}
                \item \textbf{Input}\\
                    I will text you if the train is l
            \end{itemize}
            \column{.4\linewidth}
            \begin{itemize}
                \item \textbf{Sorted completions}\\
                    \visible<2->{late}%
                    \visible<3->{, lovely}%
                    \visible<4>{, lazy}
            \end{itemize}
        \end{columns}
    \end{example}
\end{frame}

\begin{frame}[fragile]{How it is done: The easy generalization step}
    \begin{pythoncode}
        def bigrams(text):
            """Convert text to list of bigram tokens."""
            return [text[n:n+2] for n in range(len(text) - 1)]

        
        brown.words()[:5]
        >>> ["The", "Fulton", "County", "Grand", "Jury"]
        bigrams(brown.words())[:3]
        >>> [["The", "Fulton"], ["Fulton", "County"],
             ["County", "Grand"]]
    \end{pythoncode}

    \begin{pythoncode}
        brown_bigrams = bigrams(brown.words())
        total = len(brown_bigrams)
        brown_bicounts = Counter(brown_bigrams)
        for bigram in brown_bicounts:
            brown_bicounts[bigram] = brown_bicounts[bigram]/total
    \end{pythoncode}
\end{frame}

\begin{frame}[fragile]{How it is done: The trickier part}
    \begin{pythoncode}
        def bigram_completions(word, previous_word, counts):
            # set of all compatible bigrams
            comps = [comp for comp in counts
                     if comp[0] == previous_word and
                        comp[-1].startswith(word)]
            # sort the bigram completions
            ordered_ngrams = sorted(comps,
                                    key=lambda x: counts[x],
                                    reverse=True)
            # only keep second word of each bigram
            return [ngram[-1] for ngram in ordered_ngrams]
    \end{pythoncode}
\end{frame}

\begin{frame}{Linguistic evaluation}
    \begin{itemize}
        \item We now use the local context to choose word completions.
    \end{itemize}

    \begin{block}{The n-Gram Hypothesis}
        One can reliably predict the next word based on\\
        the \highlight{preceding $n-1$ words}.
    \end{block}

    \begin{itemize}
        \item The n-gram hypothesis is \highlight{not quite true}, though.
    \end{itemize}
\end{frame}

\begin{frame}{Context matters a lot}
    \begin{itemize}
        \item Words do not exist in a vacuum.

        \begin{example}
            The word \emph{hypothyroidism} is rarely heard or seen,\\
            unless you're an endocrinologist.
        \end{example}

        \item Word choices depend greatly on genre, target audience, age of the speaker, and so on.
    \end{itemize}

    \begin{block}{Common fixes}
        \begin{itemize}
            \item Use frequencies from the appropriate genre\\
                  (\textbf{BUT:} must be able to determine genre first)
            \item Learn directly from the use case\\
                  (e.g.~analyze all text messages on phone)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Long-distance dependencies in language}
    \begin{itemize}
        \item Word choice can be influenced by words that are very far away.
    \end{itemize}

    \begin{exampleblock}{Subject-verb agreement}
        \begin{itemize}
            \item The key to the cabinet \textbf{is} on the table.
            \item The keys to the cabinets \textbf{are} on the table.
            \item The key to the cabinets \textbf{is}\slash\highlight{are} on the table.
            \item The keys to the cabinet \highlight{is}\slash\textbf{are} on the table.
        \end{itemize}
    \end{exampleblock}

    \begin{itemize}
        \item Psycholinguistic observation: humans get those ``wrong'' too
        \item Potential fix: \highlight{larger n-grams}
    \end{itemize}
\end{frame}

\begin{frame}{How long can n-grams be?}
    \begin{itemize}
        \item It is tempting to move to longer and longer n-grams\\
              in order to handle long-distance dependencies.
        \item But this has two problems:
                \begin{description}
                    \item[data sparsity] longer n-grams require too much data
                    \item[storage] longer n-grams require lots of storage
                \end{description}
        \item Data sparsity is much more severe than storage.
    \end{itemize}    
\end{frame}

\begin{frame}{Sparse data: A simple calculation}
    \begin{center}
        \small
        \begin{tabular}{rcccc}
            \textbf{Words} &\textbf{bigrams} & \textbf{trigrams} & \textbf{5-grams} & \textbf{6-grams}\\
            10 & 100 & 1000 & 10,000 & 100,000\\
            100 & 10,000 & 1,000,000 & 10,000,000,000 & \Purple{1,000,000,000,000}\\
            10,000 & 10$^8$ & \Purple{10$^{12}$} & 10$^{20}$ & 10$^{24}$\\
            25,000 & $6.3 \times 10^8$ & $1.6 \times 10^{13}$ & $9.7 \times 10^{21}$ & $2.4 \times 10^{26}$\\
        \end{tabular}
    \end{center}

    \begin{block}{Some comparison values}
        \begin{description}
            \item[$4.3 \times 10^{17}$] number of seconds since the Big Bang
            \item[$5 \times 10^{22}$]   number of stars in observable universe
            \item[$10^{24}$] milliliters of water in the Earth's oceans\\
            \item[$8.8 \times 10^{26}$] diameter of observable universe, in meters
            \item[$10^{80}$] number of atoms in observable universe
        \end{description}
    \end{block}

    \begin{itemize}
        \item \textbf{Conclusion:} with large n, most n-grams are never encountered
    \end{itemize}
\end{frame}

\begin{frame}{Things get worse: A more realistic estimate}
    \begin{itemize}
        \item The Unix dictionary american-english-insane has\\
            650,000 entries.
        \item This makes the numbers much worse.\\
            Can you guess how many 5-grams there are then?
    \end{itemize}

    \begin{center}
        \visible<2->{116 octillion $\mathbf{\approx 10^{29}}$}
    \end{center}
        
    \medskip
    \visible<3>{
            $10^{29}$ is larger than the number of shotglasses it takes to\\
            drain the Earth's oceans over 2000 times.
        }
\end{frame}

\begin{frame}{Trick 1: Stemming and Lemmatization}
    \begin{itemize}
        \item Removing inflectional markers reduces number of words
        \item Two solutions:
            \begin{itemize}
                \item stemming is quick and dirty
                \item lemmatization is accurate but complex
            \end{itemize}
    \end{itemize}

    \begin{description}
        \item[stemming] cut off word ends that look like inflection
    \end{description}

    \begin{example}
        \begin{itemize}
            \item cats $\Rightarrow$ cat
            \item tasks $\Rightarrow$ task (noun and verb)
            \item asking $\Rightarrow$ ask
            \item meeting $\Rightarrow$ meet (\textbf{noun and verb})
            \item staging $\Rightarrow$ stag
        \end{itemize}
    \end{example}
\end{frame}

\begin{frame}{Trick 1: Stemming and Lemmatization [cont.]}
    \begin{description}
        \item[lemmatization] stemming with context information
    \end{description}

    \begin{example}
        \begin{itemize}
            \item cats $\Rightarrow$ cat
            \item tasks $\Rightarrow$ task (noun and verb)
            \item asking $\Rightarrow$ ask
            \item meeting $\Rightarrow$ meet (\textbf{only verb})
            \item staging $\Rightarrow$ stag\slash stage
        \end{itemize}
    \end{example}

    \begin{block}{Evaluation}
        \begin{itemize}
            \item Stemming\slash lemmatization reduces the number of words.
            \item But we still have at least 10,000 words and thus $10^{20}$ 5-grams.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Trick 2: Statistics}
    \begin{itemize}
        \item \textbf{Backoff Method}\\
            If an n-gram has frequency $0$, use the frequency of the corresponding $(n-1)$-gram.
        \item \textbf{Good-Turing Smoothing}\\
            Change frequency from $0$ to a very low value while lowering high frequency values.
    \end{itemize}

    \begin{block}{Evaluation}
        \begin{itemize}
            \item These tricks solve the issue of n-grams with 0\% frequency.
            \item But they do not solve the basic problem that\\
                  large n-gram models are incredibly data hungry.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Another problem: storage}
    % fixme
\end{frame}

\begin{frame}{Summary}
    A good word completion model needs a lot of components:

    \begin{itemize}
        \item corpora from various genres
        \item n-grams for limited context information
        \item sophisticated statistics to deal with sparse data
        \item a fast and compact data structure for storage
        \item extensions: stemming\slash lemmatizer
    \end{itemize}

    We have barely scratched the surface of any of those.
\end{frame}

\begin{frame}[fragile]{New concepts}
    \begin{itemize}
        \begin{itemize}
            \item corpora (common words, what types there are)
            \item n-grams (word = unigram)
            \item types VS tokens
            \item data sparsity
            \item smoothing and backoff
            \item stemming, tagging
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{New Python concepts}
    \begin{pythoncode}
        # New Python tricks
        sorted(some_list,
               # reverse order
               reverse=True, 
               # what property to use for sorting
               key=lambda x: something_with_x)

        range(n)  # all numbers from 0 to n-1

        some_string.startwith(x)  # does some_string start with x?
    \end{pythoncode}
\end{frame}
\end{document}
